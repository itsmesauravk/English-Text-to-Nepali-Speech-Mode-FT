{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57366a6a",
   "metadata": {},
   "source": [
    "## Fine Tuining model (tuskbyte/nepali_male_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bb93d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehorizon/jlab-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-09-23 08:41:03.981361: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-09-23 08:41:05.019685: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-23 08:41:09.509656: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from transformers import VitsModel, VitsTokenizer, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.modeling_outputs import ModelOutput\n",
    "import librosa \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset as HFDataset\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f106e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging setup\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52f9b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUDIO PROCESSING AND MEL SPECTROGRAM\n",
    "\n",
    "def audio_to_mel_spectrogram(audio, sr=16000, n_mels=80, n_fft=1024, hop_length=256, win_length=1024):\n",
    "    \"\"\"\n",
    "    Convert audio waveform to mel spectrogram\n",
    "    Args: \n",
    "        audio (np.array or torch.Tensor) : The 1D audio waveform\n",
    "        sr (int): Sampling rate of the audio in Hz. \n",
    "        n_mels (int): Number of mel frequency bands. \n",
    "        n_fft (int): Number of samples per FFT window. Determines frequency resolution.\n",
    "        hop_length(int): Number of samples to step between successive FFT windows. Controls time resolution\n",
    "        win_length(int): Size of each FFT window in samples. Typically equal to n_fft.\n",
    "\n",
    "        Returns:\n",
    "            mel_spectrogram (torch.Tensor): The mel spectrogram of shape (n_mels, time_frames)\n",
    "    \"\"\"\n",
    "    #ensure audio is in a numpy array\n",
    "    if torch.is_tensor(audio):\n",
    "        audio = audio.numpy()\n",
    "\n",
    "    # compute mel spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        win_length=win_length,\n",
    "        fmin=0.0,\n",
    "        fmax=sr / 2.0\n",
    "    )\n",
    "\n",
    "\n",
    "    # convert to log scale (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    return torch.FloatTensor(mel_spec_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83afde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# AUDIO PRE-PROCESSING\n",
    "\n",
    "def preprocess_audio(audio_path, target_sr=16000, target_db=-20):\n",
    "    \"\"\"\n",
    "    Preprocessing audio file for VITS training.\n",
    "    - Normalize sampling rate to 22050 Hz (VITS standard)\n",
    "    - Normalize volume to -20 db (consistent loudness, good for training)\n",
    "    - Trim silence from beginning/end (focus on speech)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #loading audio\n",
    "        audio, sr = librosa.load(audio_path, sr=target_sr)\n",
    "\n",
    "        #removing silence from beginning and end\n",
    "        audio, _ = librosa.effects.trim(audio, top_db=20)\n",
    "\n",
    "        # applying target db normalization\n",
    "        eps = 1e-9\n",
    "        # rms = torch.sqrt(torch.mean(torch.tensor(audio)**2))\n",
    "        rms = np.sqrt(np.mean(audio.astype(np.float64)**2) + eps)\n",
    "        if rms > 0:\n",
    "            target_rms = 10 ** (target_db/20)\n",
    "            audio = audio * (target_rms / rms)\n",
    "\n",
    "        # ensure minimun length of audio\n",
    "        min_sample = int(0.5 * target_sr)  # 0.5 seconds\n",
    "        if len(audio) < min_sample:\n",
    "            # audio = torch.nn.functional.pad(audio, (0, min_sample - len(audio)), mode='constant')\n",
    "            audio = np.pad(audio, (0, min_sample - len(audio)), mode='constant')\n",
    "        \n",
    "        # ensure maximun length of audio\n",
    "        max_sample = int(10 * target_sr)  # 10 seconds\n",
    "        if len(audio) > max_sample:\n",
    "            audio = audio[:max_sample]\n",
    "        \n",
    "        return audio, target_sr, True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        return None, None, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3c516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_model_type(model_name):\n",
    "    \"\"\"\n",
    "    Check if the model is VITS or not\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = VitsModel.from_pretrained(model_name)\n",
    "        logger.info(\"Using VITS model\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model {model_name}: {e}\")\n",
    "        raise ValueError(\"Currently only VITS model is supported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a37af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def veryify_tokenizer_compatibility(tokenizer, sample_texts):\n",
    "    \"\"\"\n",
    "    Verify that the tokenizer can works with Nepali dataset vocabulary \n",
    "    \"\"\"\n",
    "    logger.info(\"Verifying tokenizer compatibility...\")\n",
    "\n",
    "    issues = []\n",
    "    for text in sample_texts[:5]:  # check first 5 samples\n",
    "        try:\n",
    "            tokens = tokenizer(text, return_tensors='pt')\n",
    "            #check for excessive unknown tokens\n",
    "            token_ids = tokens['input_ids'].squeeze() \n",
    "\n",
    "            #count potential issues\n",
    "            if len(token_ids) == 0:\n",
    "                issues.append(f\"Empty tokenization for: {text}\")\n",
    "            \n",
    "            elif len(token_ids) > 150: #very long tokenization might indicate issues\n",
    "                issues.append(f\"Excessively long tokenization ({len(token_ids)} tokens) for: {text}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text '{text}': {e}\")\n",
    "            issues.append(f\"Error processing text '{text}': {e}\")\n",
    "\n",
    "    if issues:\n",
    "        logger.warning(f\"Tokenizer compatibility issues found: {issues}\")\n",
    "        for issue in issues:\n",
    "            logger.warning(f\"  - {issue}\")\n",
    "    else:\n",
    "        logger.info(\"Tokenizer is compatible with Nepali dataset vocabulary.\")\n",
    "\n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9044aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## DATASET WITH MEL SPECTROGRAMS\n",
    "\n",
    "class NepaliVITSDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with proper mel spectrograms computation \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_length=150, sample_rate=16000):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = sample_rate\n",
    "\n",
    "        #verify tokenizer compatibility\n",
    "        veryify_tokenizer_compatibility(tokenizer, self.data['sentence'].tolist())\n",
    "\n",
    "        logger.info(f\"Dataset initialized with {len(self.data)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        #get text and audio path\n",
    "        text = str(row['sentence']).strip()\n",
    "        audio_path = str(row['path'])\n",
    "\n",
    "        ## audio is already pre-processed, so only getting audio\n",
    "        audio, sr, _ = preprocess_audio(audio_path, target_sr=self.sample_rate)\n",
    "        # audio, sr = librosa.load(audio_path, sr=self.sample_rate)\n",
    "\n",
    "    \n",
    "        # compute mel spectrogram for proper loss computation\n",
    "        mel_spec = audio_to_mel_spectrogram(audio, sr=self.sample_rate)\n",
    "\n",
    "        #tokenize text\n",
    "        try:\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = tokenized['input_ids'].squeeze(0)  # remove batch dimension\n",
    "            attention_mask = tokenized['attention_mask'].squeeze(0) # remove batch dimension\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Error tokenizing text '{text}': {e}\")\n",
    "            #fallback\n",
    "            input_ids = torch.zeros(self.max_length, dtype=torch.long)\n",
    "            attention_mask = torch.zeros(self.max_length, dtype=torch.long)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'audio':torch.tensor(audio),\n",
    "            'mel_spectrogram': mel_spec,\n",
    "            'text': text,\n",
    "            'audio_path': audio_path\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c53ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VITSDataCollator:\n",
    "    \"\"\"\n",
    "    Data collator for VITS model, that handls mel spectrogram properly\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_audio_length=16000*8):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_audio_length = max_audio_length  #  8 seconds at 16kHz\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        #handle variable length audio and mel spectrograms\n",
    "        audio_lengths = [len(item['audio']) for item in batch]\n",
    "        max_audio_len = min(max(audio_lengths), self.max_audio_length)\n",
    "\n",
    "        #handle mel spectrograms (variable time dimension)\n",
    "        mel_time_lengths = [item['mel_spectrogram'].shape[1] for item in batch]\n",
    "        max_mel_time = max(mel_time_lengths)\n",
    "\n",
    "        #pad audio sequences\n",
    "        padded_audios = []\n",
    "        padded_mels = []\n",
    "        actual_lengths = []\n",
    "\n",
    "        for item in batch:\n",
    "            #pad audio\n",
    "            audio = item['audio']\n",
    "            actual_length = len(audio)\n",
    "\n",
    "            if actual_length > max_audio_len:\n",
    "                audio = audio[:max_audio_len]\n",
    "                actual_length = max_audio_len\n",
    "            elif actual_length < max_audio_len:\n",
    "                padding = max_audio_len - actual_length\n",
    "                audio = torch.cat([audio, torch.zeros(padding)], dim=0)\n",
    "\n",
    "            padded_audios.append(audio)\n",
    "            actual_lengths.append(actual_length)\n",
    "\n",
    "            #pad mel spectrogram\n",
    "            mel = item['mel_spectrogram'] # shape: (n_mels, time_frames)\n",
    "            mel_time = mel.shape[1]\n",
    "\n",
    "            if mel_time < max_mel_time:\n",
    "                padding = max_mel_time - mel_time\n",
    "                mel = torch.cat([mel, torch.zeros((mel.shape[0], padding))], dim=1) \n",
    "            elif mel_time > max_mel_time:\n",
    "                mel = mel[:, :max_mel_time]\n",
    "\n",
    "            padded_mels.append(mel)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.stack([item['input_ids'] for item in batch]),\n",
    "            \"attention_mask\": torch.stack([item['attention_mask'] for item in batch]),\n",
    "            \"audio\": torch.stack(padded_audios),\n",
    "            \"mel_spectrogram\": torch.stack(padded_mels),\n",
    "            # \"audio_lengths\": torch.tensor(actual_lengths),\n",
    "            \"labels\": torch.stack([item['input_ids'] for item in batch])  # for VITS, labels are same as input_ids\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c58c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VITSTrainerOutput(ModelOutput):\n",
    "    \"\"\"\n",
    "    Wrapper for VITS model outputs compatible with HuggingFace Trainer\n",
    "    \"\"\"\n",
    "    loss: torch.Tensor = None\n",
    "    waveform: torch.Tensor = None\n",
    "    logits: torch.Tensor = None  # optional, if your model returns prediction_scores\n",
    "\n",
    "    def __init__(self, loss=None, waveform=None, logits=None):\n",
    "        self.loss = loss\n",
    "        self.waveform = waveform\n",
    "        self.logits = logits\n",
    "\n",
    "\n",
    "class VITSFineTuner(nn.Module):\n",
    "    \"\"\"\n",
    "    VITS wrapper with mel spectrogram loss computation \n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"tuskbyte/nepali_male_v1\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # check model type and load appropriate base model\n",
    "        self.base_model = check_model_type(model_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "        #freeze early layers for stable fine-tuning\n",
    "        self.freeze_early_layers(num_layers_to_freeze=3)\n",
    "\n",
    "    def freeze_early_layers(self, num_layers_to_freeze=3):\n",
    "        \"\"\" \n",
    "        Better layer freezing strategy\n",
    "        \"\"\"\n",
    "        frozen_params = 0\n",
    "        total_params = 0\n",
    "\n",
    "        for name, param in self.base_model.named_parameters():\n",
    "            total_params += 1\n",
    "\n",
    "            # Freeze text encoder layers (preserve language understanding)\n",
    "            if \"text_encoder\" in name and any(f\"layers.{i}\" in name for i in range(num_layers_to_freeze)):\n",
    "                param.requires_grad = False\n",
    "                frozen_params += 1\n",
    "            \n",
    "            # Freeze early flow layers (preserve basic acoustic modeling)\n",
    "            elif \"flow\" in name and any(f\"layers.{i}\" in name for i in range(num_layers_to_freeze//2)):\n",
    "                param.requires_grad = False\n",
    "                frozen_params += 1\n",
    "\n",
    "        logger.info(f\"Frozen {frozen_params}/{total_params} parameters ({100 * frozen_params / total_params:.2f}%) for stable training.\")\n",
    "\n",
    "    \n",
    "    def compute_mel_loss(self, generated_audio, target_audio):\n",
    "        \"\"\"\n",
    "        Proper mel spectrogram loss computation \n",
    "        \"\"\"\n",
    "        try:\n",
    "            #convert both to mel spectrograms\n",
    "            generated_mel = []\n",
    "            target_mel = []\n",
    "\n",
    "            for gen_audio, tgt_audio in zip(generated_audio, target_audio):\n",
    "                #ensure same length\n",
    "                min_len = min(len(gen_audio), len(tgt_audio))\n",
    "                gen_audio = gen_audio[:min_len]\n",
    "                tgt_audio = tgt_audio[:min_len]\n",
    "\n",
    "                #convert to mel spectrogram\n",
    "                gen_mel = audio_to_mel_spectrogram(gen_audio)\n",
    "                tgt_mel = audio_to_mel_spectrogram(tgt_audio)\n",
    "\n",
    "                generated_mel.append(gen_mel)\n",
    "                target_mel.append(tgt_mel)\n",
    "\n",
    "            #pad mel spectrograms to same shape\n",
    "            max_time = max(mel.shape[1] for mel in generated_mel + target_mel)\n",
    "\n",
    "            padded_gen_mels = []\n",
    "            padded_tgt_mels = []\n",
    "\n",
    "            for gen_mel, tgt_mel in zip(generated_mel, target_mel):\n",
    "                #pad time dimension\n",
    "                if gen_mel.shape[1] < max_time:\n",
    "                    gen_mel = torch.cat([gen_mel, torch.zeros((gen_mel.shape[0], max_time - gen_mel.shape[1]))], dim=1)\n",
    "                if tgt_mel.shape[1] < max_time:\n",
    "                    tgt_mel = torch.cat([tgt_mel, torch.zeros((tgt_mel.shape[0], max_time - tgt_mel.shape[1]))], dim=1) \n",
    "                \n",
    "                padded_gen_mels.append(gen_mel)\n",
    "                padded_tgt_mels.append(tgt_mel)\n",
    "\n",
    "            \n",
    "            # stack and compute L1 loss\n",
    "            generated_mels = torch.stack(padded_gen_mels)\n",
    "            target_mels = torch.stack(padded_tgt_mels)\n",
    "\n",
    "            # Move to same device\n",
    "            if generated_audio.is_cuda:\n",
    "                target_mels = target_mels.cuda()\n",
    "                generated_mels = generated_mels.cuda()\n",
    "\n",
    "            mel_loss = F.l1_loss(generated_mels, target_mels)\n",
    "            return mel_loss\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Mel loss computation error: {e}\")\n",
    "            # fallback to simple loss\n",
    "            return F.mse_loss(generated_audio.mean(), target_audio.mean())\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, audio=None, mel_spectrogram=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Forward pass with mel spectrogram loss \n",
    "        \"\"\"\n",
    "\n",
    "        clean_kwargs = {k: v for k, v in kwargs.items() \n",
    "                   if k not in ['audio_lengths', 'audio', 'mel_spectrogram']}\n",
    "        \n",
    "        try:\n",
    "            #get model outputs\n",
    "            outputs = self.base_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                **clean_kwargs  \n",
    "            )\n",
    "\n",
    "            #compute loss if training data is provided\n",
    "            loss = None\n",
    "            if audio is not None:\n",
    "                if hasattr(outputs, 'waveform') and outputs.waveform is not None:\n",
    "                    loss = self.compute_mel_loss(outputs.waveform, audio)\n",
    "                \n",
    "                elif hasattr(outputs, 'prediction_scores'):\n",
    "                    # if models that give output mel spectrogram directly\n",
    "                    if mel_spectrogram is not None:\n",
    "                        pred_mels = outputs.prediction_scores\n",
    "                        loss = F.l1_loss(pred_mels, mel_spectrogram)\n",
    "                    else:\n",
    "                        # fallback loss \n",
    "                        loss = torch.tensor(0.01, requires_grad=True, device=input_ids.device) #dummy loss value for preventing errors in backpropagation\n",
    "                else:\n",
    "                    # create learning signal even without direct audio output\n",
    "                    # use text reconstruction loss as a proxy\n",
    "                    if hasattr(outputs, 'last_hidden_state'):\n",
    "                        #simple regularization loss to keep model learning\n",
    "                        hidden_states = outputs.last_hidden_state\n",
    "                        loss = 0.01 * torch.mean(hidden_states.pow(2))  # L2 regularization\n",
    "                    else:\n",
    "                        loss = torch.tensor(0.01, requires_grad=True, device=input_ids.device) #dummy loss value for preventing errors in backpropagation\n",
    "\n",
    "            # return outputs and loss\n",
    "            # class CustomOutput:\n",
    "            #     def __init_(self, loss, original_outputs):\n",
    "            #         self.loss = loss\n",
    "            #         #copy all original outputs attributes\n",
    "            #         for attr_name in dir(original_outputs):\n",
    "            #             if not attr_name.startswith(\"_\"):\n",
    "            #                 setattr(self, attr_name, getattr(original_outputs, attr_name))\n",
    "            \n",
    "            waveform = getattr(outputs, \"waveform\", None)\n",
    "            logits = getattr(outputs, \"prediction_scores\", None)\n",
    "\n",
    "            # Fallback: always ensure a scalar loss exists\n",
    "            if loss is None:\n",
    "                loss = torch.tensor(0.0, device=input_ids.device, requires_grad=True)\n",
    "\n",
    "            return VITSTrainerOutput(loss=loss, waveform=waveform, logits=logits)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Forward pass error: {e}\")\n",
    "            # Return stable fallback\n",
    "            dummy_loss = torch.tensor(0.01, requires_grad=True, device=input_ids.device)\n",
    "\n",
    "            # class DummyOutput:\n",
    "            #     def __init__(self, loss):\n",
    "            #         self.loss = loss\n",
    "\n",
    "            # return DummyOutput(dummy_loss)\n",
    "\n",
    "            return VITSTrainerOutput(loss=dummy_loss, waveform=None, logits=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d47ceb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TRAINING ARGUMENTS\n",
    "\n",
    "def setup_training_arguments(output_dir=\"./nepali_vits_finetuned\"):\n",
    "    \"\"\"\n",
    "    Setup training arguments with proper logging and saving strategies \n",
    "    \"\"\"\n",
    "    \n",
    "    return TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "\n",
    "        num_train_epochs=25,\n",
    "        per_device_train_batch_size=2,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "\n",
    "        #learning rate scheduler\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "        adam_beta1=0.9,\n",
    "        adam_beta2=0.999,\n",
    "        adam_epsilon=1e-8,\n",
    "\n",
    "        #scheduler\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_steps=200,\n",
    "\n",
    "        #logging and saving\n",
    "        logging_dir=f\"{output_dir}/logs\",\n",
    "        logging_steps=100,\n",
    "        save_steps=1000,\n",
    "        save_total_limit=5,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=500,\n",
    "\n",
    "        #memory and stability improvements\n",
    "        fp16=False, \n",
    "        dataloader_pin_memory=False, # this can be set to True if system has enough memory\n",
    "        dataloader_num_workers=0, \n",
    "\n",
    "        #early stopping and best model \n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "\n",
    "        #reporting\n",
    "        report_to='tensorboard',\n",
    "        run_name=f\"nepali_vits_finetune_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9d84f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## TRAINING FUNCTION\n",
    "\n",
    "def train_nepali_vits(dataframe, model_name=\"tuskbyte/nepali_male_v1\", output_dir=\"./nepali_vits_finetuned\"):\n",
    "    \"\"\"\n",
    "    Complete training function to fine-tune VITS on Nepali dataset \n",
    "    \"\"\"\n",
    "    logger.info(\"Starting Nepali VITS fine-tuning...\")\n",
    "    logger.info(\"=\"*60)\n",
    "\n",
    "    #create output dir\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # first step: load model and tokenizer\n",
    "    logger.info(\"Loading model: %s and tokenizer...\", model_name)\n",
    "\n",
    "    try:\n",
    "        tokenizer = VitsTokenizer.from_pretrained(model_name)\n",
    "        model = VITSFineTuner(model_name=model_name)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model or tokenizer: {e}\")\n",
    "        raise\n",
    "\n",
    "    #second step: prepare datasets with better splitting\n",
    "    logger.info(\"Preparing datasets...\")\n",
    "    #shuffle and split\n",
    "    shuffled_df = dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    train_size = int(0.85 * len(shuffled_df)) # 85% for training, 15% for validation\n",
    "\n",
    "    train_df = shuffled_df.iloc[:train_size]\n",
    "    val_df = shuffled_df.iloc[train_size:]\n",
    "\n",
    "    logger.info(f\"Training samples: {len(train_df)}\")\n",
    "    logger.info(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "    #create datasets\n",
    "    train_dataset = NepaliVITSDataset(train_df, tokenizer)\n",
    "    val_dataset = NepaliVITSDataset(val_df, tokenizer)\n",
    "\n",
    "    #setup data collator\n",
    "    data_collator = VITSDataCollator(tokenizer)\n",
    "\n",
    "    #third step: setup training arguments\n",
    "    logger.info(\"Setting up training arguments...\")\n",
    "    training_args = setup_training_arguments(output_dir=output_dir)\n",
    "\n",
    "    #fourth step: setup trainer with early stopping\n",
    "    logger.info(\"Initializing Trainer...\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)] #stop if no improvement in 5 eval steps\n",
    "    )\n",
    "    \n",
    "\n",
    "    #fifth step: start training\n",
    "    logger.info(\"==> Starting training...\")\n",
    "    try:\n",
    "        #clear cuda cache before training\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        trainer.train()\n",
    "        logger.info(\"==> Training completed successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Training error: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "    #final step: save final model and tokenizer\n",
    "    logger.info(\"Saving final model and tokenizer...\")\n",
    "    model.base_model.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "\n",
    "    # save training info\n",
    "    training_info = {\n",
    "        \"model_name\": model_name,\n",
    "        \"model_type\": \"VITS\",\n",
    "        \"train_samples\": len(train_df),\n",
    "        \"val_samples\": len(val_df),\n",
    "        \"final_train_loss\": trainer.state.log_history[-1].get(\"train_loss\", \"unknown\"),\n",
    "        \"final_eval_loss\": trainer.state.log_history[-1].get(\"eval_loss\", \"unknown\"),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "\n",
    "    with open(os.path.join(output_dir, \"training_info.json\"), \"w\") as f:\n",
    "        json.dump(training_info, f, indent=4)\n",
    "    \n",
    "    logger.info(\"Fine-tuning completed! Model saved to %s\", output_dir)\n",
    "\n",
    "    return trainer, model, tokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a1d71e",
   "metadata": {},
   "source": [
    "#### Making data ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a251746",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need only one time\n",
    "\n",
    "# import librosa\n",
    "# import soundfile as sf\n",
    "# from glob import glob\n",
    "\n",
    "# mp3_files = glob(\"../../data/common_voice/**/clips/*.mp3\", recursive=True)\n",
    "\n",
    "# for mp3_path in mp3_files:\n",
    "#     wav_path = mp3_path.replace(\".mp3\", \".wav\")\n",
    "\n",
    "#     # Load with librosa (auto-decoder via audioread)\n",
    "#     y, sr = librosa.load(mp3_path, sr=16000, mono=True)  # target 16kHz, mono\n",
    "\n",
    "#     # Save as wav\n",
    "#     sf.write(wav_path, y, 16000)\n",
    "\n",
    "# print(\"Conversion completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5867e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../data/common_voice/cv-corpus-20.0-2024-12...</td>\n",
       "      <td>‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§π‡§∞‡•Ç ‡§ó‡§Ø‡•ã ‡§§?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../data/common_voice/cv-corpus-21.0-2025-03...</td>\n",
       "      <td>‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§ù‡§£‡•ç‡§°‡§æ ‡§´‡§∞‡§∞ .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../data/common_voice/cv-corpus-21.0-2025-03...</td>\n",
       "      <td>‡§§‡•ç‡§Ø‡§∏ ‡§∏‡§Æ‡§ø‡§§‡§ø‡§≤‡•á ‡§π‡§æ‡§Æ‡•Ä‡§≤‡§æ‡§à ‡§≠‡•á‡§ü‡•á‡§∞ ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä ‡§ó‡§∞‡•á‡§ï‡•ã ‡§•‡§ø‡§Ø‡•ã .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../data/common_voice/cv-corpus-22.0-2025-06...</td>\n",
       "      <td>‡§ñ‡§æ‡§≤‡•Ä ‡§†‡§æ‡§â‡§Å ‡§ú‡•ã‡§ó‡§æ‡§â‡§®‡•Å‡§™‡§∞‡•ç‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§®‡§ø ‡§â‡§†‡§ø‡§∞‡§π‡•á‡§ï‡•ã ‡§õ .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../data/common_voice/cv-corpus-22.0-2025-06...</td>\n",
       "      <td>‡§Æ ‡§Æ‡•Å‡§∏‡§æ‡§ï‡•ã ‡§§‡§∞‡•ç‡§´‡§¨‡§æ‡§ü ‡§¶‡§æ‡§á .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  \\\n",
       "0  ../../data/common_voice/cv-corpus-20.0-2024-12...   \n",
       "1  ../../data/common_voice/cv-corpus-21.0-2025-03...   \n",
       "2  ../../data/common_voice/cv-corpus-21.0-2025-03...   \n",
       "3  ../../data/common_voice/cv-corpus-22.0-2025-06...   \n",
       "4  ../../data/common_voice/cv-corpus-22.0-2025-06...   \n",
       "\n",
       "                                            sentence  \n",
       "0                             ‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§π‡§∞‡•Ç ‡§ó‡§Ø‡•ã ‡§§?  \n",
       "1                                 ‡§π‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§ù‡§£‡•ç‡§°‡§æ ‡§´‡§∞‡§∞ .  \n",
       "2   ‡§§‡•ç‡§Ø‡§∏ ‡§∏‡§Æ‡§ø‡§§‡§ø‡§≤‡•á ‡§π‡§æ‡§Æ‡•Ä‡§≤‡§æ‡§à ‡§≠‡•á‡§ü‡•á‡§∞ ‡§ï‡•Å‡§∞‡§æ‡§ï‡§æ‡§®‡•Ä ‡§ó‡§∞‡•á‡§ï‡•ã ‡§•‡§ø‡§Ø‡•ã .  \n",
       "3  ‡§ñ‡§æ‡§≤‡•Ä ‡§†‡§æ‡§â‡§Å ‡§ú‡•ã‡§ó‡§æ‡§â‡§®‡•Å‡§™‡§∞‡•ç‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§Ü‡§µ‡§æ‡§ú ‡§™‡§®‡§ø ‡§â‡§†‡§ø‡§∞‡§π‡•á‡§ï‡•ã ‡§õ .  \n",
       "4                             ‡§Æ ‡§Æ‡•Å‡§∏‡§æ‡§ï‡•ã ‡§§‡§∞‡•ç‡§´‡§¨‡§æ‡§ü ‡§¶‡§æ‡§á .  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./vits_training_data/train_filelist.txt\", sep=\"|\", names=[\"path\", \"speaker_id\", \"sentence\"])\n",
    "\n",
    "# Replace .mp3 with .wav in the path column\n",
    "df[\"path\"] = df[\"path\"].str.replace(\".mp3\", \".wav\", regex=False)\n",
    "\n",
    "# Keep only useful columns\n",
    "df = df[[\"path\", \"sentence\"]]\n",
    "\n",
    "# Save back to DataFrame-ready CSV\n",
    "df.to_csv(\"train_clean.csv\", index=False)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28fa32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Starting Nepali VITS fine-tuning...\n",
      "INFO:__main__:============================================================\n",
      "INFO:__main__:Loading model: tuskbyte/nepali_male_v1 and tokenizer...\n",
      "INFO:__main__:Using VITS model\n",
      "INFO:__main__:Frozen 78/762 parameters (10.24%) for stable training.\n",
      "INFO:__main__:Preparing datasets...\n",
      "INFO:__main__:Training samples: 2398\n",
      "INFO:__main__:Validation samples: 424\n",
      "INFO:__main__:Verifying tokenizer compatibility...\n",
      "INFO:__main__:Tokenizer is compatible with Nepali dataset vocabulary.\n",
      "INFO:__main__:Dataset initialized with 2398 samples.\n",
      "INFO:__main__:Verifying tokenizer compatibility...\n",
      "INFO:__main__:Tokenizer is compatible with Nepali dataset vocabulary.\n",
      "INFO:__main__:Dataset initialized with 424 samples.\n",
      "INFO:__main__:Setting up training arguments...\n",
      "/home/ehorizon/jlab-env/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "INFO:__main__:Initializing Trainer...\n",
      "/tmp/ipykernel_4987/2151958782.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "INFO:__main__:==> Starting training...\n",
      "  0%|          | 0/7475 [00:00<?, ?it/s]WARNING:__main__:Forward pass error: Training of VITS is not supported yet.\n",
      "ERROR:__main__:Training error: The model did not return a loss from the inputs, only the following keys: . For reference, the inputs it received are input_ids,attention_mask,audio,mel_spectrogram,labels.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: . For reference, the inputs it received are input_ids,attention_mask,audio,mel_spectrogram,labels.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrain_nepali_vits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mtrain_nepali_vits\u001b[39m\u001b[34m(dataframe, model_name, output_dir)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m#clear cuda cache before training\u001b[39;00m\n\u001b[32m     66\u001b[39m     torch.cuda.empty_cache()\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33m==> Training completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jlab-env/lib/python3.12/site-packages/transformers/trainer.py:2122\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2120\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2121\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2127\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jlab-env/lib/python3.12/site-packages/transformers/trainer.py:2474\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2471\u001b[39m     \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28mself\u001b[39m.callback_handler.on_step_begin(args, \u001b[38;5;28mself\u001b[39m.state, \u001b[38;5;28mself\u001b[39m.control)\n\u001b[32m   2473\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.accumulate(model):\n\u001b[32m-> \u001b[39m\u001b[32m2474\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2477\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2478\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2479\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2480\u001b[39m ):\n\u001b[32m   2481\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2482\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jlab-env/lib/python3.12/site-packages/transformers/trainer.py:3572\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3569\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3571\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3572\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3574\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3576\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3578\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/jlab-env/lib/python3.12/site-packages/transformers/trainer.py:3646\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3644\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3645\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[32m-> \u001b[39m\u001b[32m3646\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   3647\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3648\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(outputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. For reference, the inputs it received are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m,\u001b[39m\u001b[33m'\u001b[39m.join(inputs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   3649\u001b[39m         )\n\u001b[32m   3650\u001b[39m     \u001b[38;5;66;03m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[32m   3651\u001b[39m     loss = outputs[\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: The model did not return a loss from the inputs, only the following keys: . For reference, the inputs it received are input_ids,attention_mask,audio,mel_spectrogram,labels."
     ]
    }
   ],
   "source": [
    "train_nepali_vits(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51ae8ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current device: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8743ba7c",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_finetuned_model(model_path, test_text=None):\n",
    "    \"\"\"\n",
    "    Proper inference handling for fine-tuned model \n",
    "    \"\"\"\n",
    "    if test_text is None:\n",
    "        test_text = [\n",
    "            \"‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞, ‡§Æ ‡§è‡§ï ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§¨‡•ã‡§≤‡•ç‡§õ‡•Å‡•§\",\n",
    "            \"‡§Æ‡•à‡§≤‡•á ‡§è‡§ï ‡§™‡§ü‡§ï ‡§ï‡§≤‡•á‡§ú ‡§Æ‡§æ ‡§ï‡•Å‡§∞‡§æ ‡§ó‡§∞‡•ç‡§®‡•Å ‡§™‡§∞‡•ç‡§õ‡•§\", \n",
    "            \"‡§®‡•ç‡§õ ‡§≠‡§®‡•ç‡§®‡•á ‡§∂‡§¨‡•ç‡§¶ ‡§¨‡•ã‡§≤‡•ç‡§® ‡§ó‡§æ‡§π‡•ç‡§∞‡•ã ‡§õ‡•§\",\n",
    "            \"‡§Ü‡§ú ‡§Æ‡•å‡§∏‡§Æ ‡§∞‡§æ‡§Æ‡•ç‡§∞‡•ã ‡§õ‡•§\",\n",
    "            \"‡§ß‡§®‡•ç‡§Ø‡§µ‡§æ‡§¶ ‡§∞ ‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞‡•§\"\n",
    "        ]\n",
    "\n",
    "    logger.info(\"Testing fine-tuned model\")\n",
    "\n",
    "    try:\n",
    "        #load model info\n",
    "        with open(os.path.join(model_path, \"training_info.json\"), \"r\") as f:\n",
    "            training_info = json.load(f)\n",
    "            model_type = training_info.get('model_type', 'unknown')\n",
    "\n",
    "        #load tokenizer and model\n",
    "        model = VitsModel.from_pretrained(model_path)\n",
    "        tokenizer = VitsTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        model.eval()\n",
    "        logger.info(f\"Loaded fine-tuned model (type: {model_type})\")\n",
    "\n",
    "        # generation \n",
    "        for i, text in enumerate(test_text):\n",
    "            try:\n",
    "                logger.info(f\"Generating audio for: {text}\")\n",
    "\n",
    "                inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = model(**inputs) #forward pass \n",
    "                \n",
    "                audio_generated = False\n",
    "\n",
    "                if hasattr(output, 'waveform') and output.waveform is not None:\n",
    "                    #direct waveform output\n",
    "                    audio = output.waveform.squeeze().cpu().numpy()\n",
    "                    output_path = f\"{model_path}/test_output_{i+1}.wav\"\n",
    "                    sf.write(output_path, audio, 16000)\n",
    "                    logger.info(f\"Audio saved to {output_path}\")\n",
    "                    audio_generated = True\n",
    "\n",
    "                elif hasattr(output, 'audio') and output.audio is not None:\n",
    "                    #alternative audio output\n",
    "                    audio = output.audio.squeeze().cpu().numpy()\n",
    "                    output_path = f\"{model_path}/test_output_{i+1}.wav\"\n",
    "                    sf.write(output_path, audio, 16000)\n",
    "                    logger.info(f\"Audio saved to {output_path}\")\n",
    "                    audio_generated = True\n",
    "\n",
    "                elif hasattr(output, 'last_hidden_state'):\n",
    "                    #mel spectrogram output - need vocoder\n",
    "                    logger.warning(f\"Model output is mel spectrogram. Vocoder needed for waveform generation for text: {text}.\")\n",
    "                    logger.info(\"Use a vocoder like HiFi-GAN or WaveGlow to convert mel spectrogram to waveform.\")\n",
    "\n",
    "                if not audio_generated:\n",
    "                    logger.warning(f\"No valid audio output generated for text: {text}\")\n",
    "                    logger.info(f\" Output type: {type(output)}\")\n",
    "                    logger.info(f\" Available attributes: {[attr for attr in dir(output) if not attr.startswith('_')]}\")\n",
    "\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error generating audio for text '{text}': {e}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error testing fine-tuned model: {e}\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
